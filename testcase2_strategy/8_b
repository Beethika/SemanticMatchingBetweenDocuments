Facebook published the results of a 2012 study in the Proceedings of the National Academy of Sciences. Unbeknown to users, Facebook had tampered with the news feeds of nearly 700,000 people, showing them an abnormally low number of either positive or negative posts. The experiment aimed to determine whether the company could alter the emotional state of its users.

News of the research sparked outrage from people who felt manipulated by the company.

“What many of us feared is already a reality: Facebook is using us as lab rats, and not just to figure out which ads we’ll respond to but actually change our emotions,” wrote animalnewyork.com, a blogpost that drew attention to the study in June.

The experiment was designed to assess whether more positive or negative comments in a Facebook newsfeed would impact how the user updated their own page. Facebook used an algorithm to filter content. Researchers found those shown more negative comments posted more negative comments and vice versa.

“It is clear now that there are things we should have done differently,” Schroepfer writes. “For example, we should have considered other non-experimental ways to do this research. The research would also have benefited from more extensive review by a wider and more senior group of people. Last, in releasing the study, we failed to communicate clearly why and how we did it.”
